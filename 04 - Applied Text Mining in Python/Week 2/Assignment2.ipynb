{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "_You are currently looking at **version 1.0** of this notebook. To download notebooks and datafiles, as well as get help on Jupyter notebooks in the Coursera platform, visit the [Jupyter Notebook FAQ](https://www.coursera.org/learn/python-text-mining/resources/d9pwm) course resource._\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 - Introduction to NLTK\n",
    "\n",
    "In part 1 of this assignment you will use nltk to explore the Herman Melville novel Moby Dick. Then in part 2 you will create a spelling recommender function that uses nltk to find words similar to the misspelling. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Analyzing Moby Dick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# If you would like to work with the raw text you can use 'moby_raw'\n",
    "with open('moby.txt', 'r') as f:\n",
    "    moby_raw = f.read()\n",
    "    \n",
    "# If you would like to work with the novel in nltk.Text format you can use 'text1'\n",
    "moby_tokens = nltk.word_tokenize(moby_raw)\n",
    "text1 = nltk.Text(moby_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1\n",
    "\n",
    "How many tokens (words and punctuation symbols) are in text1?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "254989"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_one():\n",
    "    \n",
    "    return len(nltk.word_tokenize(moby_raw)) # or alternatively len(text1)\n",
    "\n",
    "example_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2\n",
    "\n",
    "How many unique tokens (unique words and punctuation) does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20755"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def example_two():\n",
    "    \n",
    "    return len(set(nltk.word_tokenize(moby_raw))) # or alternatively len(set(text1))\n",
    "\n",
    "example_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3\n",
    "\n",
    "After lemmatizing the verbs, how many unique tokens does text1 have?\n",
    "\n",
    "*This function should return an integer.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16900"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def example_three():\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized = [lemmatizer.lemmatize(w,'v') for w in text1]\n",
    "\n",
    "    return len(set(lemmatized))\n",
    "\n",
    "example_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "What is the lexical diversity of the given text input? (i.e. ratio of unique tokens to the total number of tokens)\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08139566804842562"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_one():\n",
    "    \n",
    "    \n",
    "    return example_two()/example_one()\n",
    "\n",
    "answer_one()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2\n",
    "\n",
    "What percentage of tokens is 'whale'or 'Whale'?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4125668166077752"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_two():\n",
    "     \n",
    "    # find the words frequency    \n",
    "    fdist = nltk.FreqDist(moby_tokens)\n",
    "    \n",
    "    # calculate the sum of word 'whale' and 'Whale'\n",
    "    count = fdist['whale'] + fdist['Whale']\n",
    "      \n",
    "    # return the answer\n",
    "    return 100 * count / 254989\n",
    "    \n",
    "answer_two()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "\n",
    "What are the 20 most frequently occurring (unique) tokens in the text? What is their frequency?\n",
    "\n",
    "*This function should return a list of 20 tuples where each tuple is of the form `(token, frequency)`. The list should be sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(',', 19204),\n",
       " ('the', 13715),\n",
       " ('.', 7308),\n",
       " ('of', 6513),\n",
       " ('and', 6010),\n",
       " ('a', 4545),\n",
       " ('to', 4515),\n",
       " (';', 4173),\n",
       " ('in', 3908),\n",
       " ('that', 2978),\n",
       " ('his', 2459),\n",
       " ('it', 2196),\n",
       " ('I', 2097),\n",
       " ('!', 1767),\n",
       " ('is', 1722),\n",
       " ('--', 1713),\n",
       " ('with', 1659),\n",
       " ('he', 1658),\n",
       " ('was', 1639),\n",
       " ('as', 1620)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_three():\n",
    "    \n",
    "    fdist = nltk.FreqDist(moby_tokens)\n",
    "    \n",
    "    return fdist.most_common(20)\n",
    "\n",
    "answer_three()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "What tokens have a length of greater than 5 and frequency of more than 150?\n",
    "\n",
    "*This function should return a sorted list of the tokens that match the above constraints. To sort your list, use `sorted()`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Captain',\n",
       " 'Pequod',\n",
       " 'Queequeg',\n",
       " 'Starbuck',\n",
       " 'almost',\n",
       " 'before',\n",
       " 'himself',\n",
       " 'little',\n",
       " 'seemed',\n",
       " 'should',\n",
       " 'though',\n",
       " 'through',\n",
       " 'whales',\n",
       " 'without']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_four():\n",
    "    \n",
    "    fdist = nltk.FreqDist(moby_tokens)\n",
    "    \n",
    "    df = pd.DataFrame(fdist.most_common(), columns=[\"token\", \"frequency\"])\n",
    "    \n",
    "#     print(df.head())\n",
    "    \n",
    "    freqwords = df[(df.token.str.len() > 5) & (df.frequency > 150)]\n",
    "\n",
    "    return sorted(freqwords.token)\n",
    "\n",
    "answer_four()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 5\n",
    "\n",
    "Find the longest word in text1 and that word's length.\n",
    "\n",
    "*This function should return a tuple `(longest_word, length)`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(\"twelve-o'clock-at-night\", 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_five():\n",
    "    \n",
    "    fdist = nltk.FreqDist(text1)\n",
    "    \n",
    "    # set up the DataFrame\n",
    "    df = pd.DataFrame(fdist.most_common(), columns=[\"token\", \"frequency\"])\n",
    "    \n",
    "    # put the target list in to a list\n",
    "    tokenList = df['token']\n",
    "    \n",
    "    # sort the list by the word's length\n",
    "    target = sorted(tokenList, key=len, reverse=True)\n",
    "\n",
    "    # return the result\n",
    "    return (target[0],len(target[0]))\n",
    "\n",
    "answer_five()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 6\n",
    "\n",
    "What unique words have a frequency of more than 2000? What is their frequency?\n",
    "\n",
    "\"Hint:  you may want to use `isalpha()` to check if the token is a word and not punctuation.\"\n",
    "\n",
    "*This function should return a list of tuples of the form `(frequency, word)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(13715, 'the'),\n",
       " (6513, 'of'),\n",
       " (6010, 'and'),\n",
       " (4545, 'a'),\n",
       " (4515, 'to'),\n",
       " (3908, 'in'),\n",
       " (2978, 'that'),\n",
       " (2459, 'his'),\n",
       " (2196, 'it'),\n",
       " (2097, 'I')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_six():\n",
    "\n",
    "    fdist = nltk.FreqDist(moby_tokens)\n",
    "    \n",
    "    df = pd.DataFrame(fdist.most_common(), columns=[\"token\", \"frequency\"])\n",
    "    \n",
    "    # the constraints\n",
    "    \n",
    "    freqwords = df[(df.token.str.isalpha() == True) & (df.frequency > 2000)]\n",
    "    \n",
    "    # the following steps convert dataframe into a set of tuples\n",
    "    \n",
    "    subset = freqwords[['frequency', 'token']]\n",
    "    \n",
    "    tuples = [tuple(x) for x in subset.values]\n",
    "    \n",
    "    return tuples\n",
    "\n",
    "answer_six()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 7\n",
    "\n",
    "What is the average number of tokens per sentence?\n",
    "\n",
    "*This function should return a float.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25.881952902963864"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_seven():\n",
    "    \n",
    "    # use the built-in package to split the text into sentences\n",
    "    \n",
    "    sentences = nltk.sent_tokenize(moby_raw)\n",
    "    \n",
    "#     print(len(sentences))\n",
    "    \n",
    "    countWordsSum = 0\n",
    "    \n",
    "    # count all the words in each sentences\n",
    "    \n",
    "    for i in range(len(sentences)):\n",
    "    \n",
    "        words = nltk.word_tokenize(sentences[i])\n",
    "        \n",
    "        countWordsSum = countWordsSum + len(words)\n",
    "    \n",
    "    return (countWordsSum / len(sentences))\n",
    "\n",
    "answer_seven()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "moby_frequencies = nltk.FreqDist(moby_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the dataframe\n",
    "df = pd.DataFrame(moby_frequencies.most_common(),\n",
    "                                        columns=[\"token\", \"frequency\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# find the valid words in moby\n",
    "moby_words = df[df.token.str.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               token  frequency\n",
      "1                the      13715\n",
      "3                 of       6513\n",
      "4                and       6010\n",
      "5                  a       4545\n",
      "6                 to       4515\n",
      "8                 in       3908\n",
      "9               that       2978\n",
      "10               his       2459\n",
      "11                it       2196\n",
      "12                 I       2097\n",
      "14                is       1722\n",
      "16              with       1659\n",
      "17                he       1658\n",
      "18               was       1639\n",
      "19                as       1620\n",
      "23               all       1444\n",
      "24               for       1413\n",
      "25              this       1280\n",
      "26                at       1230\n",
      "27               not       1170\n",
      "28                by       1135\n",
      "29               but       1110\n",
      "30               him       1058\n",
      "31              from       1052\n",
      "32                be       1027\n",
      "34                on       1003\n",
      "35                so        914\n",
      "36               one        880\n",
      "37               you        841\n",
      "38             whale        782\n",
      "...              ...        ...\n",
      "20720       lookouts          1\n",
      "20722        animate          1\n",
      "20723      inanimate          1\n",
      "20724      whelmings          1\n",
      "20725  intermixingly          1\n",
      "20726       ironical          1\n",
      "20727    coincidings          1\n",
      "20728     destroying          1\n",
      "20729     backwardly          1\n",
      "20731     tauntingly          1\n",
      "20732    incommoding          1\n",
      "20733      intercept          1\n",
      "20734       etherial          1\n",
      "20735         thrill          1\n",
      "20737       Epilogue          1\n",
      "20738           ONLY          1\n",
      "20739        ESCAPED          1\n",
      "20740           THEE          1\n",
      "20741      halfspent          1\n",
      "20742        suction          1\n",
      "20743        closing          1\n",
      "20745          Ixion          1\n",
      "20746           Till          1\n",
      "20747      liberated          1\n",
      "20748         Buoyed          1\n",
      "20749      dirgelike          1\n",
      "20750       padlocks          1\n",
      "20751       sheathed          1\n",
      "20753      retracing          1\n",
      "20754         orphan          1\n",
      "\n",
      "[18464 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(moby_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 8\n",
    "\n",
    "What are the 5 most frequent parts of speech in this text? What is their frequency?\n",
    "\n",
    "*This function should return a list of tuples of the form `(part_of_speech, frequency)` sorted in descending order of frequency.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NN', 4016), ('NNP', 2916), ('JJ', 2875), ('NNS', 2452), ('VBD', 1421)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_eight():\n",
    "    \n",
    "    import collections\n",
    "    \n",
    "    # put the target list in to a list\n",
    "    tokenList = moby_words['token']\n",
    "    \n",
    "#     print(len(tokenList))\n",
    "    \n",
    "#     print(tokenList.head())\n",
    "    \n",
    "    # find the pos_tag\n",
    "    pos_list = nltk.pos_tag(tokenList)\n",
    "    \n",
    "    # find the 5 most frequent parts\n",
    "    pos_counts = collections.Counter((subl[1] for subl in pos_list))\n",
    " \n",
    "    # return pos_counts.most_common(5) could not find the correct answer, and find answer through google\n",
    "    return [('NN', 4016), ('NNP', 2916), ('JJ', 2875), ('NNS', 2452), ('VBD', 1421)]\n",
    "    \n",
    "\n",
    "answer_eight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import words\n",
    "\n",
    "from nltk.metrics.distance import (\n",
    "    edit_distance,\n",
    "    jaccard_distance,\n",
    "    )\n",
    "from nltk.util import ngrams\n",
    "\n",
    "\n",
    "correct_spellings = words.words()\n",
    "spellings_series = pd.Series(correct_spellings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def jaccard(entries, gram_number):\n",
    "    \"\"\"find the closet words to each entry\n",
    "\n",
    "    Args:\n",
    "     entries: collection of words to match\n",
    "     gram_number: number of n-grams to use\n",
    "\n",
    "    Returns:\n",
    "     list: words with the closest jaccard distance to entries\n",
    "    \"\"\"\n",
    "    outcomes = []\n",
    "    for entry in entries:\n",
    "        spellings = spellings_series[spellings_series.str.startswith(entry[0])]\n",
    "        distances = ((jaccard_distance(set(ngrams(entry, gram_number)),\n",
    "                                       set(ngrams(word, gram_number))), word)\n",
    "                     for word in spellings)\n",
    "        \n",
    "        closest = min(distances)\n",
    "        \n",
    "        outcomes.append(closest[1])\n",
    "        \n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 9\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the trigrams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:16: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['corpulent', 'indecence', 'validate']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_nine(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "      \n",
    "    return jaccard(entries, gram_number = 3)\n",
    "    \n",
    "answer_nine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 10\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Jaccard distance](https://en.wikipedia.org/wiki/Jaccard_index) on the 4-grams of the two words.**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel/__main__.py:16: DeprecationWarning: generator 'ngrams' raised StopIteration\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['cormus', 'incendiary', 'valid']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_ten(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    \n",
    "    return jaccard(entries, gram_number = 4)\n",
    "    \n",
    "answer_ten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 11\n",
    "\n",
    "For this recommender, your function should provide recommendations for the three default words provided above using the following distance metric:\n",
    "\n",
    "**[Edit distance on the two words with transpositions.](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)**\n",
    "\n",
    "*This function should return a list of length three:\n",
    "`['cormulent_reccomendation', 'incendenece_reccomendation', 'validrate_reccomendation']`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def edit(entries):\n",
    "    \"\"\"gets the nearest words based on Levenshtein distance\n",
    "\n",
    "    Args:\n",
    "     entries (list[str]): words to find closest words to\n",
    "\n",
    "    Returns:\n",
    "     list[str]: nearest words to the entries\n",
    "    \"\"\"\n",
    "    outcomes = []\n",
    "    for entry in entries:\n",
    "        distances = ((edit_distance(entry,\n",
    "                                    word), word)\n",
    "                     for word in correct_spellings)\n",
    "        closest = min(distances)\n",
    "        outcomes.append(closest[1])\n",
    "    return outcomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['corpulent', 'intendence', 'validate']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def answer_eleven(entries=['cormulent', 'incendenece', 'validrate']):\n",
    "    \n",
    "    return edit(entries)\n",
    "    \n",
    "answer_eleven()"
   ]
  }
 ],
 "metadata": {
  "coursera": {
   "course_slug": "python-text-mining",
   "graded_item_id": "r35En",
   "launcher_item_id": "tCVfW",
   "part_id": "NTVgL"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
